{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9005233d-e9f6-4b49-8db9-c5ce83ba292c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# INITAL HMMR Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5a0ee-e2f2-471f-aa7b-925b1ce4f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- paths & imports ---\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO, SeqRecord, Seq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, collections, re, subprocess, shlex, shutil\n",
    "\n",
    "# Your project base on Windows (WSL mount):\n",
    "BASE        = Path(\"/mnt/c/Users/SAM/CODE/HMMR\")\n",
    "MSA         = BASE / \"8ca-1024.aln-fasta\"                     # master aligned FASTA (all seqs)\n",
    "OG          = BASE / \"OG_Labels\"                              # per-class FASTA files (unaligned) e.g., *.txt\n",
    "UNIPROT_BIG = BASE / \"uniprotkb_carbonic_anhydrase_2025_08_22.fasta\"\n",
    "\n",
    "# Work/output dirs (safe to re-create):\n",
    "ALIGN_DIR   = BASE / \"per_class_aligned\"                      # aligned subsets from MSA (full set per class)\n",
    "SPLIT_DIR   = BASE / \"per_class_split\"                        # train/test ids per class\n",
    "TRAIN_ALN   = BASE / \"train_aligned\"                          # training MSAs (aligned) after slicing\n",
    "CLEAN_ALN   = BASE / \"train_aligned_clean\"                    # strict-cleaned training MSAs\n",
    "TRIM_ALN    = BASE / \"train_aligned_trimmed\"                  # training MSAs after gappy-column trimming\n",
    "PROFILES    = BASE / \"profiles\"                               # HMMs written here (on /mnt/c OK)\n",
    "RESULTS     = BASE / \"results\"\n",
    "LOGS        = BASE / \"logs\"\n",
    "TMP         = BASE / \"tmp\"\n",
    "\n",
    "# Where we'll press and keep the combined HMM library (Linux FS avoids Windows locking)\n",
    "HMM_LIB     = Path.home() / \"hmmer_lib\"                       # e.g., /home/sfkaplan/hmmer_lib\n",
    "HMM_LIB.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for d in [ALIGN_DIR, SPLIT_DIR, TRAIN_ALN, CLEAN_ALN, TRIM_ALN, PROFILES, RESULTS, LOGS, TMP]:\n",
    "    d.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Reproducible split:\n",
    "RNG_SEED    = 1337\n",
    "random.seed(RNG_SEED)\n",
    "\n",
    "def run(cmd, log=None, check=False):\n",
    "    \"\"\"Run a shell command and (optionally) write logs.\"\"\"\n",
    "    print(\"$\", cmd)\n",
    "    p = subprocess.run(shlex.split(cmd), capture_output=True, text=True)\n",
    "    if log:\n",
    "        Path(log).write_text(p.stdout + \"\\n--- STDERR ---\\n\" + p.stderr)\n",
    "    if check and p.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed ({p.returncode})\\n{p.stderr}\")\n",
    "    return p\n",
    "\n",
    "def first_token(s: str) -> str:\n",
    "    \"\"\"Return the first whitespace-delimited token from a FASTA header.\"\"\"\n",
    "    return s.split()[0] if s else s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e5bfa-fe7f-4d4b-87ee-26f5380f3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert MSA.exists(), f\"Missing master alignment: {MSA}\"\n",
    "assert OG.exists(),  f\"Missing OG_Labels dir: {OG}\"\n",
    "\n",
    "# Index the master alignment by first token of ID/description\n",
    "msa_records = list(SeqIO.parse(str(MSA), \"fasta\"))\n",
    "msa_index   = { first_token(r.id or r.description): r for r in msa_records }\n",
    "print(f\"[MSA] Loaded {len(msa_records)} aligned sequences\")\n",
    "\n",
    "rows = []\n",
    "for f in sorted(OG.glob(\"*.txt\")):\n",
    "    cls = f.stem                          # e.g., '8ca.alpha.45'\n",
    "    want_ids = [ first_token(r.id or r.description) for r in SeqIO.parse(str(f), \"fasta\") ]\n",
    "    found = [ msa_index[i] for i in want_ids if i in msa_index ]\n",
    "    out_aln = ALIGN_DIR / f\"{cls}.aln.fasta\"\n",
    "    if found:\n",
    "        SeqIO.write(found, str(out_aln), \"fasta\")\n",
    "    rows.append({\"class\": cls, \"wanted\": len(want_ids), \"found\": len(found), \"out\": str(out_aln) })\n",
    "\n",
    "slice_report = pd.DataFrame(rows).sort_values(\"class\")\n",
    "display(slice_report.head(10))\n",
    "slice_report_path = BASE / \"01_slice_report.csv\"\n",
    "slice_report.to_csv(slice_report_path, index=False)\n",
    "print(\"[OK] wrote\", slice_report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7b52e-9473-457e-8b08-5aee13157612",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "split_rows = []\n",
    "for f in sorted(OG.glob(\"*.txt\")):\n",
    "    cls = f.stem\n",
    "    ids = [ first_token(r.id or r.description) for r in SeqIO.parse(str(f), \"fasta\") ]\n",
    "    ids = sorted(set(ids))\n",
    "    if len(ids) < 3:                          # too small to split meaningfully\n",
    "        split_rows.append({\"class\":cls, \"n_total\":len(ids), \"n_train\":len(ids), \"n_test\":0, \"note\":\"too_small\"})\n",
    "        # still write a train file with all ids to avoid losing the class\n",
    "        (SPLIT_DIR / f\"{cls}.train.ids\").write_text(\"\\n\".join(ids) + (\"\\n\" if ids else \"\"))\n",
    "        (SPLIT_DIR / f\"{cls}.test.ids\").write_text(\"\")\n",
    "        continue\n",
    "\n",
    "    n_train = max(2, int(0.8 * len(ids)))     # ensure at least 2 in train\n",
    "    random.shuffle(ids)\n",
    "    train_ids = sorted(ids[:n_train])\n",
    "    test_ids  = sorted(ids[n_train:])\n",
    "    (SPLIT_DIR / f\"{cls}.train.ids\").write_text(\"\\n\".join(train_ids) + \"\\n\")\n",
    "    (SPLIT_DIR / f\"{cls}.test.ids\").write_text(\"\\n\".join(test_ids) + \"\\n\")\n",
    "    split_rows.append({\"class\":cls, \"n_total\":len(ids), \"n_train\":len(train_ids), \"n_test\":len(test_ids), \"note\":\"\"})\n",
    "\n",
    "split_report = pd.DataFrame(split_rows).sort_values(\"class\")\n",
    "display(split_report)\n",
    "split_report_path = BASE / \"02_split_report.csv\"\n",
    "split_report.to_csv(split_report_path, index=False)\n",
    "print(\"[OK] wrote\", split_report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be13a19-24d1-4bf2-98a3-dd092c839ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ALN.mkdir(exist_ok=True)\n",
    "rows=[]\n",
    "for aln in sorted(ALIGN_DIR.glob(\"*.aln.fasta\")):\n",
    "    cls = aln.stem.replace(\".aln\",\"\")\n",
    "    train_ids_path = SPLIT_DIR / f\"{cls}.train.ids\"\n",
    "    if not train_ids_path.exists():\n",
    "        rows.append({\"class\":cls, \"train_written\":0, \"note\":\"no_train_ids\"}); continue\n",
    "    train_ids = set(train_ids_path.read_text().strip().splitlines())\n",
    "    in_recs = list(SeqIO.parse(str(aln), \"fasta\"))\n",
    "    out_recs = [ r for r in in_recs if first_token(r.id or r.description) in train_ids ]\n",
    "    out_path = TRAIN_ALN / f\"{cls}.train.aln.fasta\"\n",
    "    if out_recs:\n",
    "        SeqIO.write(out_recs, str(out_path), \"fasta\")\n",
    "    rows.append({\"class\":cls, \"train_written\":len(out_recs), \"note\":\"\" if out_recs else \"empty\"})\n",
    "\n",
    "train_aln_report = pd.DataFrame(rows).sort_values(\"class\")\n",
    "display(train_aln_report.head(10))\n",
    "train_aln_report_path = BASE / \"03_train_alignment_report.csv\"\n",
    "train_aln_report.to_csv(train_aln_report_path, index=False)\n",
    "print(\"[OK] wrote\", train_aln_report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaede4e-6c2c-4968-bac8-659bda4fcb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_ALN.mkdir(exist_ok=True)\n",
    "valid = set(\"ACDEFGHIKLMNPQRSTVWYBXZ-\")  # dash for gaps; allow X\n",
    "\n",
    "def clean_strict(seq_str: str) -> str:\n",
    "    s = seq_str.upper().replace(\".\", \"-\")\n",
    "    return \"\".join(ch if ch in valid else \"X\" for ch in s)\n",
    "\n",
    "def enforce_modal_length(records):\n",
    "    lengths = [len(r.seq) for r in records]\n",
    "    if not lengths: return [], 0, 0\n",
    "    modal_len, _ = collections.Counter(lengths).most_common(1)[0]\n",
    "    kept = [r for r in records if len(r.seq) == modal_len]\n",
    "    return kept, modal_len, len(records) - len(kept)\n",
    "\n",
    "rows=[]\n",
    "for aln in sorted(TRAIN_ALN.glob(\"*.train.aln.fasta\")):\n",
    "    cls = aln.stem.replace(\".train.aln\",\"\")\n",
    "    recs = []\n",
    "    repl = 0\n",
    "    for r in SeqIO.parse(str(aln), \"fasta\"):\n",
    "        raw = str(r.seq)\n",
    "        cleaned = clean_strict(raw)\n",
    "        repl += sum(1 for a,b in zip(raw.upper(), cleaned) if a != b)\n",
    "        recs.append(SeqRecord.SeqRecord(Seq.Seq(cleaned),\n",
    "                                        id=first_token(r.id or r.description),\n",
    "                                        description=\"\"))\n",
    "    kept, modal_len, dropped = enforce_modal_length(recs)\n",
    "    out = CLEAN_ALN / f\"{cls}.train.clean.aln.fasta\"\n",
    "    if kept:\n",
    "        SeqIO.write(kept, str(out), \"fasta\")\n",
    "    rows.append({\"class\":cls,\"n_input\":len(recs),\"n_kept\":len(kept),\n",
    "                 \"dropped\":dropped,\"modal_len\":modal_len,\"replacements\":repl})\n",
    "\n",
    "clean_report = pd.DataFrame(rows).sort_values(\"class\")\n",
    "display(clean_report.head(10))\n",
    "clean_report_path = BASE / \"04_clean_report.csv\"\n",
    "clean_report.to_csv(clean_report_path, index=False)\n",
    "print(\"[OK] wrote\", clean_report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0db9e6-dcca-4fc4-8e93-c7cb1fceba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIM_ALN.mkdir(exist_ok=True)\n",
    "\n",
    "def trim_alignment(in_fa: Path, out_fa: Path, min_symbol_frac=0.10):\n",
    "    recs = list(SeqIO.parse(str(in_fa), \"fasta\"))\n",
    "    if not recs:\n",
    "        return {\"class\": in_fa.stem, \"kept_cols\": 0, \"orig_cols\": 0, \"nseq\": 0}\n",
    "    arr = np.array([list(str(r.seq)) for r in recs])  # shape (N, L)\n",
    "    gapmask = (arr == \"-\")\n",
    "    symfrac = 1.0 - gapmask.mean(axis=0)\n",
    "    keep = symfrac >= min_symbol_frac\n",
    "    kept_cols = int(keep.sum()); L = arr.shape[1]\n",
    "    if kept_cols == 0:\n",
    "        keep = symfrac > 0.0\n",
    "        kept_cols = int(keep.sum())\n",
    "    if kept_cols == 0:\n",
    "        return {\"class\": in_fa.stem, \"kept_cols\": 0, \"orig_cols\": L, \"nseq\": len(recs)}\n",
    "    trimmed = [\"\".join(row[keep]) for row in arr]\n",
    "    out_recs = [SeqRecord.SeqRecord(Seq.Seq(s), id=first_token(r.id or r.description), description=\"\")\n",
    "                for s, r in zip(trimmed, recs)]\n",
    "    SeqIO.write(out_recs, str(out_fa), \"fasta\")\n",
    "    return {\"class\": in_fa.stem, \"kept_cols\": kept_cols, \"orig_cols\": L, \"nseq\": len(recs)}\n",
    "\n",
    "rows=[]\n",
    "for aln in sorted(CLEAN_ALN.glob(\"*.train.clean.aln.fasta\")):\n",
    "    out = TRIM_ALN / aln.name.replace(\".clean\", \"\")\n",
    "    rows.append(trim_alignment(aln, out, min_symbol_frac=0.10))\n",
    "\n",
    "trim_report = pd.DataFrame(rows).sort_values(\"class\")\n",
    "display(trim_report.head(10))\n",
    "trim_report_path = BASE / \"05_trim_report.csv\"\n",
    "trim_report.to_csv(trim_report_path, index=False)\n",
    "print(\"[OK] wrote\", trim_report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4461d89-c0fd-4a74-8cc3-53da1e8363f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-class HMMs with 80% dropout (keep columns with >=20% symbols); fallback to 0.0 if needed.\n",
    "records = []\n",
    "for aln in sorted(TRIM_ALN.glob(\"*.train.aln.fasta\")):\n",
    "    cls = aln.name.replace(\".train.aln.fasta\",\"\")           # clean class name\n",
    "    hmm = PROFILES / f\"{cls}.hmm\"\n",
    "    log1= LOGS / f\"{cls}.hmmbuild.log\"\n",
    "    log2= LOGS / f\"{cls}.hmmbuild.retry.log\"\n",
    "\n",
    "    # quick sanity\n",
    "    nseq  = sum(1 for _ in SeqIO.parse(str(aln), \"fasta\"))\n",
    "    nsyms = sum((c != '-') for r in SeqIO.parse(str(aln), \"fasta\") for c in str(r.seq))\n",
    "    if nseq < 2 or nsyms == 0:\n",
    "        records.append({\"class\":cls,\"nseq\":nseq,\"symbols\":nsyms,\"status\":\"skip\",\"note\":\"too_small_or_empty\"})\n",
    "        continue\n",
    "\n",
    "    # NOTE: HMMER uses -n (short) not --name\n",
    "    p1 = run(f'hmmbuild --amino --symfrac 0.2 -n \"{cls}\" --cpu 4 \"{hmm}\" \"{aln}\"', log=str(log1))\n",
    "    if p1.returncode == 0 and hmm.exists() and hmm.stat().st_size > 0:\n",
    "        records.append({\"class\":cls,\"nseq\":nseq,\"symbols\":nsyms,\"status\":\"ok\",\"note\":\"symfrac=0.2\"})\n",
    "        continue\n",
    "\n",
    "    p2 = run(f'hmmbuild --amino --symfrac 0.0 -n \"{cls}\" --cpu 4 \"{hmm}\" \"{aln}\"', log=str(log2))\n",
    "    if p2.returncode == 0 and hmm.exists() and hmm.stat().st_size > 0:\n",
    "        records.append({\"class\":cls,\"nseq\":nseq,\"symbols\":nsyms,\"status\":\"ok\",\"note\":\"fallback symfrac=0.0\"})\n",
    "    else:\n",
    "        records.append({\"class\":cls,\"nseq\":nseq,\"symbols\":nsyms,\"status\":\"fail\",\"note\":\"see logs\"})\n",
    "\n",
    "hmr = pd.DataFrame(records).sort_values([\"status\",\"class\"])\n",
    "display(hmr)\n",
    "hmr_path = BASE / \"06_hmmbuild_report.csv\"\n",
    "hmr.to_csv(hmr_path, index=False)\n",
    "print(\"[OK] wrote\", hmr_path)\n",
    "\n",
    "# Combine to a single HMM lib on /mnt/c then copy to Linux FS and press there\n",
    "combined = PROFILES / \"all_classes.hmm\"\n",
    "if combined.exists(): combined.unlink()\n",
    "parts = [p for p in sorted(PROFILES.glob(\"*.hmm\")) if p.name != \"all_classes.hmm\" and p.stat().st_size > 0]\n",
    "assert parts, \"No non-empty HMMs built; check 06_hmmbuild_report.csv and logs.\"\n",
    "\n",
    "with open(combined, \"w\") as out:\n",
    "    for p in parts:\n",
    "        s = p.read_text()\n",
    "        out.write(s if s.endswith(\"\\n\") else s + \"\\n\")\n",
    "print(f\"[combine] {len(parts)} models -> {combined} ({combined.stat().st_size} bytes)\")\n",
    "\n",
    "# Copy to Linux FS & hmmpress to avoid Windows file locking\n",
    "dst = HMM_LIB / \"all_classes.hmm\"\n",
    "shutil.copy2(combined, dst)\n",
    "\n",
    "# Clean any stale indices at destination and press\n",
    "for ext in (\".h3f\",\".h3i\",\".h3m\",\".h3p\"):\n",
    "    p = Path(str(dst) + ext)\n",
    "    if p.exists(): p.unlink()\n",
    "\n",
    "press = run(f'hmmpress \"{dst}\"', log=str(LOGS / \"hmmpress_all_classes.log\"))\n",
    "if press.returncode != 0:\n",
    "    print(Path(LOGS / \"hmmpress_all_classes.log\").read_text()[:1000])\n",
    "    raise SystemError(\"hmmpress failed; see log.\")\n",
    "\n",
    "ALL_HMM = dst   # <- use this path in later hmmscan calls\n",
    "print(\"[hmmpress] OK at\", ALL_HMM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7b447-7e25-4d4a-8dd9-8b13c236067d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Robust held-out validation (replaces previous Cell 7) ===\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "import pandas as pd, re, subprocess, shlex\n",
    "\n",
    "BASE     = Path(\"/mnt/c/Users/SAM/CODE/HMMR\")\n",
    "OG       = BASE / \"OG_Labels\"\n",
    "SPLIT    = BASE / \"per_class_split\"\n",
    "TMP      = BASE / \"tmp\";     TMP.mkdir(exist_ok=True)\n",
    "RESULTS  = BASE / \"results\"; RESULTS.mkdir(exist_ok=True)\n",
    "LOGS     = BASE / \"logs\";    LOGS.mkdir(exist_ok=True)\n",
    "\n",
    "ALL_HMM  = Path.home() / \"hmmer_lib\" / \"all_classes.hmm\"  # pressed in Linux home\n",
    "\n",
    "def first_token(s: str) -> str:\n",
    "    return s.split()[0] if s else s\n",
    "\n",
    "# 1) Gather the complete set of held-out (test) IDs per class\n",
    "test_map = {}   # seq_id -> true_class\n",
    "perclass_counts = []\n",
    "for f in sorted(OG.glob(\"*.txt\")):\n",
    "    cls = f.stem\n",
    "    tid_path = SPLIT / f\"{cls}.test.ids\"\n",
    "    if not tid_path.exists():\n",
    "        perclass_counts.append({\"class\": cls, \"n_test\": 0}); continue\n",
    "    ids = [ln.strip() for ln in tid_path.read_text().splitlines() if ln.strip()]\n",
    "    for sid in ids:\n",
    "        test_map[sid] = cls\n",
    "    perclass_counts.append({\"class\": cls, \"n_test\": len(ids)})\n",
    "\n",
    "perclass_counts = pd.DataFrame(perclass_counts).sort_values(\"class\")\n",
    "total_test = sum(perclass_counts[\"n_test\"])\n",
    "print(f\"[held-out] total test sequences requested: {total_test}\")\n",
    "display(perclass_counts.head(20))\n",
    "\n",
    "# 2) Build a FASTA with ALL held-out sequences\n",
    "TEST_MERGED = TMP / \"heldout_test_sequences.faa\"\n",
    "with open(TEST_MERGED, \"w\") as out:\n",
    "    for f in sorted(OG.glob(\"*.txt\")):\n",
    "        cls = f.stem\n",
    "        tid_path = SPLIT / f\"{cls}.test.ids\"\n",
    "        if not tid_path.exists(): continue\n",
    "        want = set(x.strip() for x in tid_path.read_text().splitlines() if x.strip())\n",
    "        if not want: continue\n",
    "        for rec in SeqIO.parse(str(f), \"fasta\"):\n",
    "            sid = first_token(rec.id or rec.description)\n",
    "            if sid in want:\n",
    "                rec.id = sid\n",
    "                rec.description = sid\n",
    "                SeqIO.write(rec, out, \"fasta\")\n",
    "\n",
    "# Sanity check: how many actually written?\n",
    "n_written = sum(1 for _ in SeqIO.parse(str(TEST_MERGED), \"fasta\"))\n",
    "print(f\"[held-out] wrote {n_written} sequences to {TEST_MERGED}\")\n",
    "\n",
    "# 3) hmmscan against the pressed library\n",
    "tbl = RESULTS / \"hmmscan_heldout.tbl\"\n",
    "dom = RESULTS / \"hmmscan_heldout.domtbl\"\n",
    "cmd = f'hmmscan --cpu 4 --tblout \"{tbl}\" --domtblout \"{dom}\" \"{ALL_HMM}\" \"{TEST_MERGED}\"'\n",
    "print(\"$\", cmd)\n",
    "subprocess.run(shlex.split(cmd), check=True)\n",
    "\n",
    "# 4) Robust parse of --tblout and pick best hit per sequence (if any)\n",
    "colnames = [\n",
    "    \"target\",\"tacc\",\"tlen\",\"query\",\"qacc\",\"qlen\",\n",
    "    \"fs_evalue\",\"fs_score\",\"fs_bias\",\n",
    "    \"n_dom\",\"n_dom_exp\",\"dom_cevalue\",\"dom_ievalue\",\n",
    "    \"hmmfrom\",\"hmmto\",\"alifrom\",\"alito\",\"envfrom\",\"envto\",\"acc\",\"desc\"\n",
    "]\n",
    "hits = pd.read_csv(tbl, sep=r\"\\s+\", comment=\"#\", header=None, names=colnames, engine=\"python\")\n",
    "\n",
    "# Keep essentials; sometimes 'query' can be '-' if parse breaks — filter those out\n",
    "hits = hits.loc[hits[\"query\"].notna() & (hits[\"query\"] != \"-\"), [\"query\",\"target\",\"fs_score\",\"fs_evalue\"]]\n",
    "hits = hits.rename(columns={\"query\":\"seq_id\",\"target\":\"pred_class\",\"fs_score\":\"bit_score\",\"fs_evalue\":\"evalue\"})\n",
    "\n",
    "# Best hit per sequence\n",
    "if not hits.empty:\n",
    "    ranked = hits.sort_values([\"seq_id\",\"bit_score\"], ascending=[True,False])\n",
    "    best_hits = ranked.groupby(\"seq_id\", as_index=False).first()\n",
    "else:\n",
    "    best_hits = pd.DataFrame(columns=[\"seq_id\",\"pred_class\",\"bit_score\",\"evalue\"])\n",
    "\n",
    "# 5) Join with the full list of test IDs so NO-HIT seqs are included\n",
    "truth = pd.Series(test_map, name=\"true_class\")\n",
    "full = pd.DataFrame({\"seq_id\": list(test_map.keys())}).merge(best_hits, on=\"seq_id\", how=\"left\")\n",
    "\n",
    "# Mark no-hit rows explicitly\n",
    "full[\"pred_class\"] = full[\"pred_class\"].fillna(\"NO_HIT\")\n",
    "# optional: set missing scores to NaN (already NaN)\n",
    "# Compute correctness (NO_HIT counts as incorrect)\n",
    "full = full.join(truth, on=\"seq_id\")\n",
    "full[\"correct\"] = (full[\"pred_class\"] == full[\"true_class\"])\n",
    "\n",
    "# 6) Metrics + artifacts\n",
    "n_eval      = len(full)\n",
    "n_hits      = int((full[\"pred_class\"] != \"NO_HIT\").sum())\n",
    "coverage    = n_hits / n_eval if n_eval else float(\"nan\")\n",
    "accuracy    = full[\"correct\"].mean() if n_eval else float(\"nan\")\n",
    "\n",
    "print(f\"[held-out] evaluated: {n_eval}  |  with ≥1 hit: {n_hits}  (coverage={coverage:.3f})\")\n",
    "print(f\"[held-out] accuracy:  {accuracy:.3f}\")\n",
    "\n",
    "# Confusion matrix including NO_HIT column\n",
    "cm = pd.crosstab(full[\"true_class\"], full[\"pred_class\"])\n",
    "display(cm.head(20))\n",
    "\n",
    "# Save reports\n",
    "full_out = BASE / \"07_heldout_predictions.csv\"\n",
    "cm_out   = BASE / \"07_confusion_matrix.csv\"\n",
    "cov_out  = BASE / \"07_heldout_summary.txt\"\n",
    "full.to_csv(full_out, index=False)\n",
    "cm.to_csv(cm_out)\n",
    "cov_out.write_text(\n",
    "    f\"total_test={total_test}\\n\"\n",
    "    f\"evaluated={n_eval}\\n\"\n",
    "    f\"with_hits={n_hits}\\n\"\n",
    "    f\"coverage={coverage:.4f}\\n\"\n",
    "    f\"accuracy={accuracy:.4f}\\n\"\n",
    ")\n",
    "print(\"[OK] wrote\", full_out, cm_out, cov_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7fe620-5f82-4e67-9b8f-97057ebb4878",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TEST Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295816c-2d6c-4b63-b9de-3d5e742e4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Held-out validation for ALPHA ONLY ===\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "import pandas as pd, subprocess, shlex\n",
    "\n",
    "BASE     = Path(\"/mnt/c/Users/SAM/CODE/HMMR\")\n",
    "OG       = BASE / \"OG_Labels\"\n",
    "SPLIT    = BASE / \"per_class_split\"\n",
    "TMP      = BASE / \"tmp\";     TMP.mkdir(exist_ok=True)\n",
    "RESULTS  = BASE / \"results\"; RESULTS.mkdir(exist_ok=True)\n",
    "\n",
    "# HMM library (pressed) – keep all classes so we can detect off-target hits\n",
    "ALL_HMM  = Path.home() / \"hmmer_lib\" / \"all_classes.hmm\"\n",
    "\n",
    "# Configuration: evaluate only alpha subclasses\n",
    "CLASS_PREFIX = \"8ca.alpha.\"\n",
    "\n",
    "def first_token(s: str) -> str:\n",
    "    \"\"\"Make sequence IDs consistent with the IDs in *.test.ids files.\"\"\"\n",
    "    return s.split()[0] if s else s\n",
    "\n",
    "# 1) Collect ALPHA held-out IDs (true labels)\n",
    "test_map = {}   # seq_id -> true_class (e.g., 8ca.alpha.45)\n",
    "perclass_counts = []\n",
    "for f in sorted(OG.glob(\"*.txt\")):\n",
    "    cls = f.stem\n",
    "    if not cls.startswith(CLASS_PREFIX):\n",
    "        continue  # skip non-alpha classes\n",
    "    tid_path = SPLIT / f\"{cls}.test.ids\"\n",
    "    if not tid_path.exists():\n",
    "        perclass_counts.append({\"class\": cls, \"n_test\": 0})\n",
    "        continue\n",
    "    ids = [ln.strip() for ln in tid_path.read_text().splitlines() if ln.strip()]\n",
    "    for sid in ids:\n",
    "        test_map[sid] = cls\n",
    "    perclass_counts.append({\"class\": cls, \"n_test\": len(ids)})\n",
    "\n",
    "perclass_counts = pd.DataFrame(perclass_counts).sort_values(\"class\")\n",
    "total_test = int(perclass_counts[\"n_test\"].sum()) if not perclass_counts.empty else 0\n",
    "print(f\"[alpha held-out] total test sequences requested: {total_test}\")\n",
    "display(perclass_counts)\n",
    "\n",
    "# 2) Build a FASTA containing only ALPHA held-out sequences\n",
    "TEST_MERGED = TMP / \"heldout_alpha_test_sequences.faa\"\n",
    "with open(TEST_MERGED, \"w\") as out:\n",
    "    for f in sorted(OG.glob(\"*.txt\")):\n",
    "        cls = f.stem\n",
    "        if not cls.startswith(CLASS_PREFIX):\n",
    "            continue\n",
    "        tid_path = SPLIT / f\"{cls}.test.ids\"\n",
    "        if not tid_path.exists():\n",
    "            continue\n",
    "        want = set(x.strip() for x in tid_path.read_text().splitlines() if x.strip())\n",
    "        if not want:\n",
    "            continue\n",
    "        for rec in SeqIO.parse(str(f), \"fasta\"):\n",
    "            sid = first_token(rec.id or rec.description)\n",
    "            if sid in want:\n",
    "                rec.id = sid\n",
    "                rec.description = sid\n",
    "                SeqIO.write(rec, out, \"fasta\")\n",
    "\n",
    "# Sanity check: how many actually written?\n",
    "n_written = sum(1 for _ in SeqIO.parse(str(TEST_MERGED), \"fasta\"))\n",
    "print(f\"[alpha held-out] wrote {n_written} sequences to {TEST_MERGED}\")\n",
    "\n",
    "# 3) hmmscan against the complete profile library\n",
    "tbl = RESULTS / \"hmmscan_alpha_heldout.tbl\"\n",
    "dom = RESULTS / \"hmmscan_alpha_heldout.domtbl\"\n",
    "cmd = f'hmmscan --cpu 4 --tblout \"{tbl}\" --domtblout \"{dom}\" \"{ALL_HMM}\" \"{TEST_MERGED}\"'\n",
    "print(\"$\", cmd)\n",
    "subprocess.run(shlex.split(cmd), check=True)\n",
    "# 4) Robust parse of --domtblout and pick best full-seq hit per sequence\n",
    "# HMMER 3.4 domtblout columns (per manual):\n",
    "# HMMER 3.4 domtblout columns (hmmscan)\n",
    "dom_cols = [\n",
    "    \"target\",\"tacc\",\"tlen\",\n",
    "    \"query\",\"qacc\",\"qlen\",\n",
    "    \"fs_evalue\",\"fs_score\",\"fs_bias\",\n",
    "    \"#\",\"of\",                         # <— the two you were missing\n",
    "    \"dom_cevalue\",\"dom_ievalue\",\"dom_score\",\"dom_bias\",\n",
    "    \"hmmfrom\",\"hmmto\",\"alifrom\",\"alito\",\"envfrom\",\"envto\",\"acc\",\n",
    "    \"desc\"\n",
    "]\n",
    "\n",
    "dom_hits = pd.read_csv(\n",
    "    dom,\n",
    "    sep=r\"\\s+\",\n",
    "    comment=\"#\",\n",
    "    header=None,\n",
    "    engine=\"python\",\n",
    "    names=dom_cols,\n",
    "    usecols=list(range(0, 23))  # everything up to (but not including) desc\n",
    ")\n",
    "\n",
    "# Keep essentials & pick best full-seq hit per query\n",
    "dom_hits = dom_hits.rename(columns={\n",
    "    \"query\":\"seq_id\", \"target\":\"pred_class\",\n",
    "    \"fs_score\":\"bit_score\", \"fs_evalue\":\"evalue\"\n",
    "})[[\"seq_id\",\"pred_class\",\"bit_score\",\"evalue\"]]\n",
    "\n",
    "ranked = dom_hits.sort_values([\"seq_id\",\"bit_score\"], ascending=[True, False])\n",
    "best_hits = ranked.groupby(\"seq_id\", as_index=False).first()\n",
    "\n",
    "\n",
    "# Best full-seq hit per sequence\n",
    "if not dom_hits.empty:\n",
    "    ranked = dom_hits.sort_values([\"seq_id\", \"bit_score\"], ascending=[True, False])\n",
    "    best_hits = ranked.groupby(\"seq_id\", as_index=False).first()\n",
    "else:\n",
    "    best_hits = pd.DataFrame(columns=[\"seq_id\", \"pred_class\", \"bit_score\", \"evalue\"])\n",
    "\n",
    "# 5) (unchanged) Join with the full list of test IDs so NO-HIT seqs are included\n",
    "truth = pd.Series(test_map, name=\"true_class\")\n",
    "full = pd.DataFrame({\"seq_id\": list(test_map.keys())}).merge(best_hits, on=\"seq_id\", how=\"left\")\n",
    "full[\"pred_class\"] = full[\"pred_class\"].fillna(\"NO_HIT\")\n",
    "full = full.join(truth, on=\"seq_id\")\n",
    "full[\"correct\"] = (full[\"pred_class\"] == full[\"true_class\"])\n",
    "\n",
    "\n",
    "# 6) Metrics + confusion matrix (restricted to ALPHA true classes)\n",
    "n_eval   = len(full)\n",
    "n_hits   = int((full[\"pred_class\"] != \"NO_HIT\").sum())\n",
    "coverage = (n_hits / n_eval) if n_eval else float(\"nan\")\n",
    "accuracy = full[\"correct\"].mean() if n_eval else float(\"nan\")\n",
    "\n",
    "print(f\"[alpha held-out] evaluated: {n_eval}  |  with ≥1 hit: {n_hits}  (coverage={coverage:.3f})\")\n",
    "print(f\"[alpha held-out] accuracy:  {accuracy:.3f}\")\n",
    "\n",
    "# Confusion matrix: rows = true alpha subclass, columns = predicted model (incl. NO_HIT)\n",
    "cm = pd.crosstab(full[\"true_class\"], full[\"pred_class\"])\n",
    "display(cm)\n",
    "\n",
    "# 7) Save artifacts\n",
    "full_out = BASE / \"07_alpha_heldout_predictions.csv\"\n",
    "cm_out   = BASE / \"07_alpha_confusion_matrix.csv\"\n",
    "sum_out  = BASE / \"07_alpha_heldout_summary.txt\"\n",
    "\n",
    "full.to_csv(full_out, index=False)\n",
    "cm.to_csv(cm_out)\n",
    "sum_out.write_text(\n",
    "    f\"total_test={total_test}\\n\"\n",
    "    f\"evaluated={n_eval}\\n\"\n",
    "    f\"with_hits={n_hits}\\n\"\n",
    "    f\"coverage={coverage:.4f}\\n\"\n",
    "    f\"accuracy={accuracy:.4f}\\n\"\n",
    ")\n",
    "print(\"[OK] wrote\", full_out, cm_out, sum_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1e801-afb4-48cc-805d-45fe750ed8d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TEsst All AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31fb92-af08-4a46-a188-924a7649e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Robust held-out validation (ALL classes) ===\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "import pandas as pd, subprocess, shlex\n",
    "\n",
    "BASE     = Path(\"/mnt/c/Users/SAM/CODE/HMMR\")\n",
    "OG       = BASE / \"OG_Labels\"\n",
    "SPLIT    = BASE / \"per_class_split\"\n",
    "TMP      = BASE / \"tmp\";     TMP.mkdir(exist_ok=True)\n",
    "RESULTS  = BASE / \"results\"; RESULTS.mkdir(exist_ok=True)\n",
    "LOGS     = BASE / \"logs\";    LOGS.mkdir(exist_ok=True)\n",
    "\n",
    "ALL_HMM  = Path.home() / \"hmmer_lib\" / \"all_classes.hmm\"  # pressed in Linux home\n",
    "\n",
    "def first_token(s: str) -> str:\n",
    "    return s.split()[0] if s else s\n",
    "\n",
    "# 1) Gather the complete set of held-out (test) IDs per class\n",
    "test_map = {}   # seq_id -> true_class\n",
    "perclass_counts = []\n",
    "for f in sorted(OG.glob(\"*.txt\")):\n",
    "    cls = f.stem\n",
    "    tid_path = SPLIT / f\"{cls}.test.ids\"\n",
    "    if not tid_path.exists():\n",
    "        perclass_counts.append({\"class\": cls, \"n_test\": 0})\n",
    "        continue\n",
    "    ids = [ln.strip() for ln in tid_path.read_text().splitlines() if ln.strip()]\n",
    "    for sid in ids:\n",
    "        test_map[sid] = cls\n",
    "    perclass_counts.append({\"class\": cls, \"n_test\": len(ids)})\n",
    "\n",
    "perclass_counts = pd.DataFrame(perclass_counts).sort_values(\"class\")\n",
    "total_test = int(perclass_counts[\"n_test\"].sum()) if not perclass_counts.empty else 0\n",
    "print(f\"[held-out] total test sequences requested: {total_test}\")\n",
    "display(perclass_counts.head(20))\n",
    "\n",
    "# 2) Build a FASTA with ALL held-out sequences\n",
    "TEST_MERGED = TMP / \"heldout_test_sequences.faa\"\n",
    "with open(TEST_MERGED, \"w\") as out:\n",
    "    for f in sorted(OG.glob(\"*.txt\")):\n",
    "        cls = f.stem\n",
    "        tid_path = SPLIT / f\"{cls}.test.ids\"\n",
    "        if not tid_path.exists():\n",
    "            continue\n",
    "        want = set(x.strip() for x in tid_path.read_text().splitlines() if x.strip())\n",
    "        if not want:\n",
    "            continue\n",
    "        for rec in SeqIO.parse(str(f), \"fasta\"):\n",
    "            sid = first_token(rec.id or rec.description)\n",
    "            if sid in want:\n",
    "                rec.id = sid\n",
    "                rec.description = sid\n",
    "                SeqIO.write(rec, out, \"fasta\")\n",
    "\n",
    "# Sanity check: how many actually written?\n",
    "n_written = sum(1 for _ in SeqIO.parse(str(TEST_MERGED), \"fasta\"))\n",
    "print(f\"[held-out] wrote {n_written} sequences to {TEST_MERGED}\")\n",
    "\n",
    "# 3) hmmscan against the pressed library\n",
    "tbl = RESULTS / \"hmmscan_heldout.tbl\"\n",
    "dom = RESULTS / \"hmmscan_heldout.domtbl\"\n",
    "cmd = f'hmmscan --cpu 4 --tblout \"{tbl}\" --domtblout \"{dom}\" \"{ALL_HMM}\" \"{TEST_MERGED}\"'\n",
    "print(\"$\", cmd)\n",
    "subprocess.run(shlex.split(cmd), check=True)\n",
    "\n",
    "# 4) Robust parse of --domtblout and pick best full-seq hit per sequence\n",
    "# HMMER 3.4 domtblout columns (hmmscan)\n",
    "dom_cols = [\n",
    "    \"target\",\"tacc\",\"tlen\",\n",
    "    \"query\",\"qacc\",\"qlen\",\n",
    "    \"fs_evalue\",\"fs_score\",\"fs_bias\",\n",
    "    \"#\",\"of\",\n",
    "    \"dom_cevalue\",\"dom_ievalue\",\"dom_score\",\"dom_bias\",\n",
    "    \"hmmfrom\",\"hmmto\",\"alifrom\",\"alito\",\"envfrom\",\"envto\",\"acc\",\n",
    "    \"desc\"\n",
    "]\n",
    "\n",
    "dom_hits = pd.read_csv(\n",
    "    dom,\n",
    "    sep=r\"\\s+\",\n",
    "    comment=\"#\",\n",
    "    header=None,\n",
    "    engine=\"python\",\n",
    "    names=dom_cols,\n",
    "    usecols=list(range(0, 23))  # exclude 'desc' which may contain spaces\n",
    ")\n",
    "\n",
    "# Keep essentials; pick best full-seq hit per query\n",
    "dom_hits = dom_hits.rename(columns={\n",
    "    \"query\":\"seq_id\",\n",
    "    \"target\":\"pred_class\",\n",
    "    \"fs_score\":\"bit_score\",\n",
    "    \"fs_evalue\":\"evalue\"\n",
    "})[[\"seq_id\",\"pred_class\",\"bit_score\",\"evalue\"]]\n",
    "\n",
    "if not dom_hits.empty:\n",
    "    ranked = dom_hits.sort_values([\"seq_id\", \"bit_score\"], ascending=[True, False])\n",
    "    best_hits = ranked.groupby(\"seq_id\", as_index=False).first()\n",
    "else:\n",
    "    best_hits = pd.DataFrame(columns=[\"seq_id\", \"pred_class\", \"bit_score\", \"evalue\"])\n",
    "\n",
    "# 5) Join with the full list of test IDs so NO-HIT seqs are included\n",
    "truth = pd.Series(test_map, name=\"true_class\")\n",
    "full = pd.DataFrame({\"seq_id\": list(test_map.keys())}).merge(best_hits, on=\"seq_id\", how=\"left\")\n",
    "\n",
    "# Mark no-hit rows explicitly; compute correctness\n",
    "full[\"pred_class\"] = full[\"pred_class\"].fillna(\"NO_HIT\")\n",
    "full = full.join(truth, on=\"seq_id\")\n",
    "full[\"correct\"] = (full[\"pred_class\"] == full[\"true_class\"])\n",
    "\n",
    "# 6) Metrics + artifacts\n",
    "n_eval      = len(full)\n",
    "n_hits      = int((full[\"pred_class\"] != \"NO_HIT\").sum())\n",
    "coverage    = n_hits / n_eval if n_eval else float(\"nan\")\n",
    "accuracy    = full[\"correct\"].mean() if n_eval else float(\"nan\")\n",
    "\n",
    "print(f\"[held-out] evaluated: {n_eval}  |  with ≥1 hit: {n_hits}  (coverage={coverage:.3f})\")\n",
    "print(f\"[held-out] accuracy:  {accuracy:.3f}\")\n",
    "\n",
    "# Confusion matrix including NO_HIT column\n",
    "cm = pd.crosstab(full[\"true_class\"], full[\"pred_class\"])\n",
    "display(cm.head(20))\n",
    "\n",
    "# 7) Save reports\n",
    "full_out = BASE / \"07_heldout_predictions.csv\"\n",
    "cm_out   = BASE / \"07_confusion_matrix.csv\"\n",
    "cov_out  = BASE / \"07_heldout_summary.txt\"\n",
    "\n",
    "full.to_csv(full_out, index=False)\n",
    "cm.to_csv(cm_out)\n",
    "cov_out.write_text(\n",
    "    f\"total_test={total_test}\\n\"\n",
    "    f\"evaluated={n_eval}\\n\"\n",
    "    f\"with_hits={n_hits}\\n\"\n",
    "    f\"coverage={coverage:.4f}\\n\"\n",
    "    f\"accuracy={accuracy:.4f}\\n\"\n",
    ")\n",
    "print(\"[OK] wrote\", full_out, cm_out, cov_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c0348-4d46-44ba-aa8f-794c1f91e5ec",
   "metadata": {},
   "source": [
    "# Label More sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0063e1-0cad-4f92-b1eb-48bbd0c82f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Label the first N UniProt sequences with HMMER (minimal CSV) ===\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "import pandas as pd, subprocess, shlex\n",
    "\n",
    "BASE        = Path(\"/mnt/c/Users/SAM/CODE/HMMR\")\n",
    "UNIPROT_BIG = BASE / \"uniprotkb_carbonic_anhydrase_2025_08_22.fasta\"\n",
    "N_TOP       = 100000  # <-- change as needed\n",
    "\n",
    "TMP      = BASE / \"tmp\";     TMP.mkdir(exist_ok=True)\n",
    "RESULTS  = BASE / \"results\"; RESULTS.mkdir(exist_ok=True)\n",
    "ALL_HMM  = Path.home() / \"hmmer_lib\" / \"all_classes.hmm\"  # pressed HMM library\n",
    "\n",
    "def first_token(s: str) -> str:\n",
    "    return s.split()[0] if s else s\n",
    "\n",
    "# 1) Subset first N sequences\n",
    "SUBSET_FASTA = TMP / f\"uniprot_first_{N_TOP}.faa\"\n",
    "subset_records = []\n",
    "for i, rec in enumerate(SeqIO.parse(str(UNIPROT_BIG), \"fasta\"), start=1):\n",
    "    subset_records.append(rec)\n",
    "    if i >= N_TOP:\n",
    "        break\n",
    "SeqIO.write(subset_records, str(SUBSET_FASTA), \"fasta\")\n",
    "print(f\"[label] wrote {len(subset_records)} sequences to {SUBSET_FASTA}\")\n",
    "\n",
    "# 2) Run hmmscan\n",
    "tbl = RESULTS / f\"hmmscan_uniprot_first_{N_TOP}.tbl\"\n",
    "dom = RESULTS / f\"hmmscan_uniprot_first_{N_TOP}.domtbl\"\n",
    "cmd = f'hmmscan --cpu 4 --tblout \"{tbl}\" --domtblout \"{dom}\" \"{ALL_HMM}\" \"{SUBSET_FASTA}\"'\n",
    "print(\"$\", cmd)\n",
    "subprocess.run(shlex.split(cmd), check=True)\n",
    "\n",
    "# 3) Parse domtblout (stable format)\n",
    "dom_cols = [\n",
    "    \"target\",\"tacc\",\"tlen\",\n",
    "    \"query\",\"qacc\",\"qlen\",\n",
    "    \"fs_evalue\",\"fs_score\",\"fs_bias\",\n",
    "    \"#\",\"of\",\n",
    "    \"dom_cevalue\",\"dom_ievalue\",\"dom_score\",\"dom_bias\",\n",
    "    \"hmmfrom\",\"hmmto\",\"alifrom\",\"alito\",\"envfrom\",\"envto\",\"acc\",\n",
    "    \"desc\"\n",
    "]\n",
    "dom_hits = pd.read_csv(\n",
    "    dom, sep=r\"\\s+\", comment=\"#\", header=None, engine=\"python\",\n",
    "    names=dom_cols, usecols=list(range(0,23))\n",
    ")\n",
    "\n",
    "# keep only essentials\n",
    "dom_hits = dom_hits.rename(columns={\n",
    "    \"query\":\"seq_id\",\"target\":\"pred_class\",\n",
    "    \"fs_score\":\"bit_score\",\"fs_evalue\":\"evalue\",\n",
    "    \"qlen\":\"query_len\",\"tlen\":\"target_len\"\n",
    "})[[\"seq_id\",\"pred_class\",\"bit_score\",\"evalue\",\"query_len\",\"target_len\"]]\n",
    "\n",
    "# best hit per sequence\n",
    "if not dom_hits.empty:\n",
    "    ranked = dom_hits.sort_values([\"seq_id\",\"bit_score\"], ascending=[True,False])\n",
    "    best_hits = ranked.groupby(\"seq_id\", as_index=False).first()\n",
    "else:\n",
    "    best_hits = pd.DataFrame(columns=[\"seq_id\",\"pred_class\",\"bit_score\",\"evalue\",\"query_len\",\"target_len\"])\n",
    "\n",
    "# 4) Join with metadata for all input sequences\n",
    "rows = []\n",
    "for rec in subset_records:\n",
    "    sid = first_token(rec.id or rec.description)\n",
    "    rows.append({\n",
    "        \"seq_id\": sid,\n",
    "        \"description\": rec.description,\n",
    "        \"length\": len(rec.seq),\n",
    "    })\n",
    "meta_df = pd.DataFrame(rows)\n",
    "\n",
    "full = meta_df.merge(best_hits, on=\"seq_id\", how=\"left\")\n",
    "full[\"pred_class\"] = full[\"pred_class\"].fillna(\"NO_HIT\")\n",
    "\n",
    "# 5) Save CSV\n",
    "out_csv = BASE / f\"08_uniprot_first_{N_TOP}_labels.csv\"\n",
    "full.to_csv(out_csv, index=False)\n",
    "print(\"[OK] wrote\", out_csv)\n",
    "\n",
    "#display(full.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2602493-a147-4b74-9e0a-6db5e4bedef5",
   "metadata": {},
   "source": [
    "# Fix no hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7612506-0d94-4d61-9de7-f19512565497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Rescue pass for NO_HITs (relaxed hmmscan), robust ===\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "import pandas as pd, subprocess, shlex, sys\n",
    "\n",
    "BASE        = Path(\"/mnt/c/Users/SAM/CODE/HMMR\")\n",
    "UNIPROT_BIG = BASE / \"uniprotkb_carbonic_anhydrase_2025_08_22.fasta\"\n",
    "RESULTS     = BASE / \"results\"; RESULTS.mkdir(exist_ok=True)\n",
    "TMP         = BASE / \"tmp\";     TMP.mkdir(exist_ok=True)\n",
    "ALL_HMM     = Path.home() / \"hmmer_lib\" / \"all_classes.hmm\"\n",
    "\n",
    "FIRST_PASS_CSV = BASE / f\"08_uniprot_first_{N_TOP}_labels.csv\"  # <-- your existing first pass\n",
    "\n",
    "def first_token(s: str) -> str:\n",
    "    return s.split()[0] if s else s\n",
    "\n",
    "def parse_domtbl(dom_path: Path) -> pd.DataFrame:\n",
    "    dom_cols = [\n",
    "        \"target\",\"tacc\",\"tlen\",\n",
    "        \"query\",\"qacc\",\"qlen\",\n",
    "        \"fs_evalue\",\"fs_score\",\"fs_bias\",\n",
    "        \"#\",\"of\",\n",
    "        \"dom_cevalue\",\"dom_ievalue\",\"dom_score\",\"dom_bias\",\n",
    "        \"hmmfrom\",\"hmmto\",\"alifrom\",\"alito\",\"envfrom\",\"envto\",\"acc\",\n",
    "        \"desc\"\n",
    "    ]\n",
    "    if not dom_path.exists() or dom_path.stat().st_size == 0:\n",
    "        return pd.DataFrame(columns=[\"seq_id\",\"pred_class\",\"bit_score\",\"evalue\",\"query_len\",\"target_len\"])\n",
    "    df = pd.read_csv(\n",
    "        dom_path, sep=r\"\\s+\", comment=\"#\", header=None, engine=\"python\",\n",
    "        names=dom_cols, usecols=list(range(0,23))\n",
    "    )\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"seq_id\",\"pred_class\",\"bit_score\",\"evalue\",\"query_len\",\"target_len\"])\n",
    "    df = df.rename(columns={\n",
    "        \"query\":\"seq_id\", \"target\":\"pred_class\",\n",
    "        \"fs_score\":\"bit_score\", \"fs_evalue\":\"evalue\",\n",
    "        \"qlen\":\"query_len\", \"tlen\":\"target_len\"\n",
    "    })[[\"seq_id\",\"pred_class\",\"bit_score\",\"evalue\",\"query_len\",\"target_len\"]]\n",
    "    ranked = df.sort_values([\"seq_id\",\"bit_score\"], ascending=[True, False])\n",
    "    return ranked.groupby(\"seq_id\", as_index=False).first()\n",
    "\n",
    "def hmmscan_relaxed(dom_out: Path, query_fa: Path) -> pd.DataFrame:\n",
    "    tbl_out = dom_out.with_suffix(\".tbl\")\n",
    "    # Correct: per-seq threshold has short form (-E), per-domain is long (--domE)\n",
    "    cmd = f'hmmscan --cpu 4 --tblout \"{tbl_out}\" --domtblout \"{dom_out}\" --max -E 100 --domE 100 \"{ALL_HMM}\" \"{query_fa}\"'\n",
    "    print(\"$\", cmd)\n",
    "    proc = subprocess.run(shlex.split(cmd), text=True, capture_output=True)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"[hmmscan stderr]\\n\" + (proc.stderr or \"(no stderr)\"))\n",
    "        print(\"[hmmscan stdout]\\n\" + (proc.stdout or \"(no stdout)\"))\n",
    "        raise subprocess.CalledProcessError(proc.returncode, cmd, output=proc.stdout, stderr=proc.stderr)\n",
    "    return parse_domtbl(dom_out)\n",
    "\n",
    "\n",
    "# 1) Load first-pass labels\n",
    "first_df = pd.read_csv(FIRST_PASS_CSV)\n",
    "assert {\"seq_id\",\"pred_class\"}.issubset(first_df.columns), \"first-pass CSV missing columns\"\n",
    "\n",
    "# 2) Gather NO_HITs\n",
    "nohit_ids = set(first_df.loc[first_df[\"pred_class\"] == \"NO_HIT\", \"seq_id\"])\n",
    "if not nohit_ids:\n",
    "    print(\"[rescue] nothing to rescue — no NO_HITs\")\n",
    "else:\n",
    "    # 3) Rebuild rescue FASTA straight from UNIPROT_BIG\n",
    "    rescue_fa = TMP / \"uniprot_nohit_rescue.faa\"\n",
    "    written = 0\n",
    "    # also collect a small sample of UniProt IDs for debugging\n",
    "    uniprot_ids_sample = []\n",
    "    with open(rescue_fa, \"w\") as out:\n",
    "        for i, rec in enumerate(SeqIO.parse(str(UNIPROT_BIG), \"fasta\")):\n",
    "            sid = first_token(rec.id or rec.description)\n",
    "            if i < 1000:  # sample first 1000 for debugging\n",
    "                uniprot_ids_sample.append(sid)\n",
    "            if sid in nohit_ids:\n",
    "                rec.id = sid\n",
    "                rec.description = sid\n",
    "                SeqIO.write(rec, out, \"fasta\")\n",
    "                written += 1\n",
    "    print(f\"[rescue] wrote {written} NO_HIT sequences to {rescue_fa}\")\n",
    "\n",
    "    # 3a) Guard: if zero written, explain why and stop cleanly\n",
    "    if written == 0:\n",
    "        missing_preview = sorted(list(nohit_ids - set(uniprot_ids_sample)))[:10]\n",
    "        print(\"[rescue] 0 sequences written. Likely the seq_id normalization differs between first-pass CSV and UniProt FASTA.\")\n",
    "        print(\"          Example NO_HIT ids not seen among first ~1000 UniProt headers:\", missing_preview)\n",
    "        print(\"          First few NO_HIT ids:\", sorted(list(nohit_ids))[:10])\n",
    "        print(\"          First few UniProt ids found:\", uniprot_ids_sample[:10])\n",
    "        print(\"          Ensure both steps use first_token() on IDs.\")\n",
    "        # Optionally: re-scan ALL of UniProt to confirm (can be slow); uncomment if needed:\n",
    "        # all_uniprot_ids = { first_token(rec.id or rec.description) for rec in SeqIO.parse(str(UNIPROT_BIG), \"fasta\") }\n",
    "        # print(\"          Example NO_HIT ids not present in full UniProt:\", sorted(list(nohit_ids - all_uniprot_ids))[:10])\n",
    "    else:\n",
    "        # 4) Relaxed hmmscan on the NO_HITs\n",
    "        dom_relaxed = RESULTS / \"hmmscan_uniprot_rescue.domtbl\"\n",
    "        rescued = hmmscan_relaxed(dom_relaxed, rescue_fa)\n",
    "\n",
    "        # 5) Acceptance rule (tune to taste)\n",
    "        def accept(row):\n",
    "            try:\n",
    "                e = float(row[\"evalue\"])\n",
    "            except Exception:\n",
    "                return False\n",
    "            bs = float(row[\"bit_score\"]) if pd.notna(row[\"bit_score\"]) else -1e9\n",
    "            return (e <= 1e-2) or (bs >= 25.0)\n",
    "\n",
    "        rescued_kept = rescued.loc[rescued.apply(accept, axis=1)]\n",
    "\n",
    "        # 6) Merge back into first_df\n",
    "        if not rescued_kept.empty:\n",
    "            rescued_map = rescued_kept.set_index(\"seq_id\")[[\"pred_class\",\"bit_score\",\"evalue\",\"query_len\",\"target_len\"]]\n",
    "            mask = first_df[\"pred_class\"].eq(\"NO_HIT\") & first_df[\"seq_id\"].isin(rescued_map.index)\n",
    "            for col in [\"pred_class\",\"bit_score\",\"evalue\",\"query_len\",\"target_len\"]:\n",
    "                first_df.loc[mask, col] = first_df.loc[mask, \"seq_id\"].map(rescued_map[col])\n",
    "            print(f\"[rescue] accepted {int(mask.sum())} upgraded labels\")\n",
    "        else:\n",
    "            print(\"[rescue] no acceptable hits found under relaxed settings\")\n",
    "\n",
    "        # 7) Save updated CSV\n",
    "        out_csv = FIRST_PASS_CSV.with_name(FIRST_PASS_CSV.stem + \"_rescued.csv\")\n",
    "        first_df.to_csv(out_csv, index=False)\n",
    "        print(\"[OK] wrote\", out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2593d-3e06-4e82-8c85-0d498a20f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have the DataFrame loaded\n",
    "df = pd.read_csv(out_csv)\n",
    "\n",
    "# count NO_HITs in the pred_class column\n",
    "nohit_count = (df[\"pred_class\"] == \"NO_HIT\").sum()\n",
    "print(nohit_count)\n",
    "\n",
    "# or proportion\n",
    "nohit_frac = (df[\"pred_class\"] == \"NO_HIT\").mean()\n",
    "print(nohit_frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ab404-14a1-4aff-b255-4cf7fd9740ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hmmer_env)",
   "language": "python",
   "name": "hmmer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
